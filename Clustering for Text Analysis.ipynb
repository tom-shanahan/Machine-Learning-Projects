{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra data file science2k-doc-word.npy contains a 1373x5476 matrix, where each row\n",
    "is an article in Science described by 5476 word features. The articles and words are in the\n",
    "same order as in the vocabulary and titles files above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1373, 5476)\n",
      "(5476, 1373)\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "doc_word = np.load(\"science2k-doc-word.npy\")\n",
    "word_doc = np.load(\"science2k-word-doc.npy\")\n",
    "\n",
    "titles = pd.read_table('science2k-titles.txt').values\n",
    "vocab = pd.read_table('science2k-vocab.txt').values\n",
    "\n",
    "print(doc_word.shape)\n",
    "print(word_doc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the documents using k-means and various values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_doc_word = []\n",
    "inertia_word_doc = []\n",
    "\n",
    "for n in range(1, 30):\n",
    "    kmean = KMeans(n_clusters = n).fit(doc_word)\n",
    "    inertia_doc_word.append(kmean.inertia_)\n",
    "    kmean = KMeans(n_clusters = n).fit(word_doc)\n",
    "    inertia_word_doc.append(kmean.inertia_)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1,30),inertia_doc_word)\n",
    "ax.plot(range(1,30),inertia_word_doc)\n",
    "plt.legend(['doc-word','word-doc'])\n",
    "plt.xlabel(\"K-Value\")\n",
    "plt.ylabel(\"Sum Distance to Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seven seems to be a good choice for K. This seems to be about the number at which the sum of the distance to the mean begins to become less steep. \n",
    "\n",
    "The top 10 words of each cluster in order of the largest positive distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "kmeans = KMeans(n_clusters = num_clusters).fit(doc_word)\n",
    "clusters = kmeans.cluster_centers_\n",
    "center = np.mean(doc_word,axis=0).reshape(1, -1)\n",
    "labels = kmeans.predict(doc_word)\n",
    "\n",
    "for cluster in range(0, num_clusters):\n",
    "    distance = euclidean_distances(doc_word[labels == cluster], center)\n",
    "    largest_10_pos_dist = np.argsort(-distance, axis = 0)[:10] \n",
    "    words = []\n",
    "    for dist in largest_10_pos_dist:\n",
    "        words.append(vocab[dist][0][0])\n",
    "    print ('Cluster: %s, Furthest 10 Words: %s' % (cluster, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ten documents that fall closest to each cluster center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(0, num_clusters):\n",
    "    distance = euclidean_distances(doc_word[labels == cluster], clusters[cluster].reshape(1, -1))\n",
    "    closest_10_titles = np.argsort(-distance, axis = 0)[:10]     \n",
    "    acticle_titles = []\n",
    "    for dist in closest_10_titles:\n",
    "        acticle_titles.append(titles[dist][0][0])\n",
    "    print ('Cluster: %s, Top 10 Titles: %s' %(cluster, acticle_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first algorithm has captured the ten words in each cluster that a farthest from the mean word. In other words, these are the words in each cluster that are most \"different\" from the mean word. This could useful in identify atypical or unusual words in each cluster.\n",
    "### The second algorithm has captured the ten documents that are closest to each cluster center. These are the documents that are most strongly represented by each cluster. If the clusters are able to distill document topic, this algorithm may be helpful for grouping documents by topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file science2k-word-doc.txt is similar, but captures term-wise rather than document-wise\n",
    "features. That is, for each term, we count the frequency as the number of documents that\n",
    "term appears in rather than the other way around. This allows us to characterize individual\n",
    "terms.\n",
    "This matrix is 5476x1373, where each row is a term in Science described by 1373 “document”\n",
    "features. These are transformed document frequencies (as above). Below we will repeat the analysis above,\n",
    "but cluster terms instead of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "kmeans = KMeans(n_clusters = num_clusters).fit(word_doc)\n",
    "clusters = kmeans.cluster_centers_\n",
    "center = np.mean(word_doc,axis=0).reshape(1, -1)\n",
    "labels = kmeans.predict(word_doc)\n",
    "\n",
    "for cluster in range(0, num_clusters):\n",
    "    distance = euclidean_distances(word_doc[labels == cluster], center)\n",
    "    largest_10_pos_dist = np.argsort(-distance, axis = 0)[:10] \n",
    "    words = []\n",
    "    for dist in largest_10_pos_dist:\n",
    "        words.append(vocab[dist][0][0])\n",
    "    print ('Cluster: %s, Furthest 10 Words: %s' % (cluster, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(0, num_clusters):\n",
    "    distance = euclidean_distances(word_doc[labels == cluster], clusters[cluster].reshape(1, -1))\n",
    "    closest_10_titles = np.argsort(-distance, axis = 0)[:10]     \n",
    "    acticle_titles = []\n",
    "    for dist in closest_10_titles:\n",
    "        acticle_titles.append(titles[dist][0][0])\n",
    "    print ('Cluster: %s, Top 10 Titles: %s' %(cluster, acticle_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first algorithm has captured the ten words in each cluster (clustered by frequency of documents they appear in) that a furthest from the mean word. In other words, these are the words in each cluster that are most \"different\" from the mean word. This could useful in identify atypical words in each cluster. \n",
    "### The second algorithm has captured the ten documents that closest to each cluster center. These are the documents that are most strongly represented by each cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
